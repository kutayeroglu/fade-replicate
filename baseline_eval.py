# -*- coding: utf-8 -*-
"""baseline-eval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SOnwJnKEEKemlywR-ThEVU9t9_bDxdR8
"""

from google.colab import drive
drive.mount('/content/drive')

"""Load data"""

import os
import shutil

# Paths in Google Drive
drive_coco_base_dir = '/content/drive/MyDrive/coco'
drive_val_images_dir = os.path.join(drive_coco_base_dir, 'images', 'val2017')
drive_val_annotations_file = os.path.join(drive_coco_base_dir, 'annotations', 'instances_val2017.json')

# Local paths
local_coco_base_dir = '/content/coco'
local_val_images_dir = os.path.join(local_coco_base_dir, 'images', 'val2017')
local_val_annotations_dir = os.path.join(local_coco_base_dir, 'annotations')

# Create local directories
os.makedirs(local_val_images_dir, exist_ok=True)
os.makedirs(local_val_annotations_dir, exist_ok=True)

# Copy images
!rsync -ah --progress "{drive_val_images_dir}/" "{local_val_images_dir}/"

# Copy annotations
shutil.copy2(drive_val_annotations_file, local_val_annotations_dir)

# Paths to the validation images and annotations in local storage
val_images_dir = local_val_images_dir
val_annotations_file = os.path.join(local_val_annotations_dir, 'instances_val2017.json')

import torch
from torchvision import datasets, transforms

# Define any image transformations if needed
transform = transforms.Compose([
    transforms.ToTensor()
])

# Initialize the COCO validation dataset
val_dataset = datasets.CocoDetection(
    root=val_images_dir,
    annFile=val_annotations_file,
    transform=transform
)

# Adjust batch_size and num_workers
batch_size = 64  # Increase if your GPU can handle it
num_workers = 4  # Increase to the number of CPU cores available

# Create a DataLoader for the validation dataset
val_loader = torch.utils.data.DataLoader(
    val_dataset,
    batch_size=batch_size,
    shuffle=False,
    num_workers=num_workers,
    collate_fn=lambda x: tuple(zip(*x))
)

# Get a single batch from the DataLoader
images, targets = next(iter(val_loader))

# Move images to GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
images = [image.to(device) for image in images]

# Check the shapes and types
print(f'Number of images in batch: {len(images)}')
print(f'Type of images: {type(images[0])}')
print(f'Image tensor shape: {images[0].shape}')
print(f'Number of targets in batch: {len(targets)}')
print(f'Type of a target: {type(targets[0])}')

import matplotlib.pyplot as plt
import numpy as np

# Move the first image back to CPU for visualization
image = images[0].cpu().permute(1, 2, 0).numpy()

plt.imshow(image)
plt.axis('off')
plt.show()

# Print annotations for the first image
print('Annotations:', targets[0])

from torchvision import models

# Load the pre-trained model
model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)

# Set the model to evaluation mode
model.eval()

model.to(device)

!pip install pycocotools

from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval

import json
import time
import numpy as np
from tqdm.notebook import tqdm

class COCODatasetWithIDs(datasets.CocoDetection):
    def __init__(self, root, annFile, transform=None):
        # Initialize the base class with transform=None
        super().__init__(root, annFile, transform=None)
        self.transform = transform

    def __getitem__(self, index):
        # Get the image and target without transformation
        img, target = super().__getitem__(index)
        img_id = self.ids[index]
        if self.transform is not None:
            img = self.transform(img)
        return img, target, img_id



val_dataset = COCODatasetWithIDs(
    root=val_images_dir,
    annFile=val_annotations_file,
    transform=transforms.ToTensor()
)

def collate_fn(batch):
    return tuple(zip(*batch))


val_loader = torch.utils.data.DataLoader(
    val_dataset,
    batch_size=1,  # Use batch_size=1 for accurate image IDs
    shuffle=False,
    num_workers=2,  # Adjusted based on the warning
    collate_fn=collate_fn
)

# Test a single batch
images, targets, image_ids = next(iter(val_loader))

print(f'Number of images in batch: {len(images)}')
print(f'Image tensor shape: {images[0].shape}')
print(f'Image ID: {image_ids[0]}')
print(f'Target type: {type(targets[0])}')

def prepare_for_coco_detection(predictions, image_ids):
    coco_results = []
    for prediction, image_id in zip(predictions, image_ids):
        boxes = prediction["boxes"]
        scores = prediction["scores"]
        labels = prediction["labels"]

        boxes = boxes.cpu().numpy()
        scores = scores.cpu().numpy()
        labels = labels.cpu().numpy()

        # Convert boxes from [xmin, ymin, xmax, ymax] to [x, y, width, height]
        boxes[:, 2:] -= boxes[:, :2]

        for box, score, label in zip(boxes, scores, labels):
            coco_result = {
                "image_id": int(image_id),
                "category_id": int(label),
                "bbox": box.tolist(),
                "score": float(score)
            }
            coco_results.append(coco_result)
    return coco_results

# Initialize COCO ground truth API
coco_gt = COCO(val_annotations_file)

# Collect model predictions
results = []

# Start inference
model.eval()
with torch.no_grad():
    for images, targets, image_ids in tqdm(val_loader):
        # Move images to device
        images = list(image.to(device) for image in images)

        # Run inference
        outputs = model(images)

        # Move outputs to CPU
        outputs = [{k: v.cpu() for k, v in t.items()} for t in outputs]

        # Prepare results for COCO evaluation
        results.extend(prepare_for_coco_detection(outputs, image_ids))

results_file = 'coco_results.json'
with open(results_file, 'w') as f:
    json.dump(results, f, indent=4)

# Load results
coco_dt = coco_gt.loadRes(results_file)

# Initialize COCOeval object
coco_evaluator = COCOeval(coco_gt, coco_dt, iouType='bbox')

# Run evaluation
coco_evaluator.evaluate()
coco_evaluator.accumulate()
coco_evaluator.summarize()

import sys
import io

# Redirect stdout to capture the summary
old_stdout = sys.stdout
sys.stdout = mystdout = io.StringIO()

coco_evaluator.summarize()

# Reset stdout
sys.stdout = old_stdout

# Get the summary string
evaluation_summary = mystdout.getvalue()

# Save the summary to a text file
metrics_file = 'baseline_metrics.txt'
with open(metrics_file, 'w') as f:
    f.write(evaluation_summary)

output_dir = '/content/drive/MyDrive/FADE/results/object-detection/baseline'
os.makedirs(output_dir, exist_ok=True)

# Copy files to Google Drive
shutil.copy('baseline_metrics.txt', output_dir)
shutil.copy('coco_results.json', output_dir)

print(f"Files saved to: {output_dir}")

# Get category-wise AP
category_ids = coco_gt.getCatIds()
categories = coco_gt.loadCats(category_ids)
category_names = [cat['name'] for cat in categories]

# Initialize a list to store per-category AP
per_category_ap = []

for idx, cat_id in enumerate(category_ids):
    coco_evaluator.params.catIds = [cat_id]
    coco_evaluator.evaluate()
    coco_evaluator.accumulate()
    coco_evaluator.summarize()
    # AP is stored in stats[0]
    per_category_ap.append(coco_evaluator.stats[0])

import matplotlib.pyplot as plt
import matplotlib.patches as patches
from PIL import Image

def plot_image_with_boxes(img, targets=None, predictions=None, score_threshold=0.5):
    """
    Plots an image with ground truth and predicted bounding boxes.

    Args:
        img (PIL.Image or torch.Tensor): The image to display.
        targets (list): Ground truth annotations.
        predictions (dict): Model predictions.
        score_threshold (float): Minimum score for displaying predicted boxes.
    """
    if isinstance(img, torch.Tensor):
        img = img.permute(1, 2, 0).cpu().numpy()

    fig, ax = plt.subplots(1, figsize=(12, 8))
    ax.imshow(img)

    # Plot ground truth boxes
    if targets is not None:
        for ann in targets:
            bbox = ann['bbox']
            category_id = ann['category_id']
            category_name = coco_gt.loadCats(category_id)[0]['name']
            rect = patches.Rectangle(
                (bbox[0], bbox[1]), bbox[2], bbox[3],
                linewidth=2, edgecolor='green', facecolor='none'
            )
            ax.add_patch(rect)
            ax.text(
                bbox[0], bbox[1] - 5, category_name,
                color='green', fontsize=12, backgroundcolor='black'
            )

    # Plot predicted boxes
    if predictions is not None:
        boxes = predictions['boxes'].cpu().numpy()
        scores = predictions['scores'].cpu().numpy()
        labels = predictions['labels'].cpu().numpy()
        for bbox, score, label in zip(boxes, scores, labels):
            if score < score_threshold:
                continue
            category_name = coco_gt.loadCats(int(label))[0]['name']
            rect = patches.Rectangle(
                (bbox[0], bbox[1]), bbox[2] - bbox[0], bbox[3] - bbox[1],
                linewidth=2, edgecolor='red', facecolor='none'
            )
            ax.add_patch(rect)
            ax.text(
                bbox[0], bbox[1] - 5, f'{category_name}: {score:.2f}',
                color='red', fontsize=12, backgroundcolor='black'
            )

    plt.axis('off')
    plt.show()

def get_image_and_prediction(idx):
    import os
    from PIL import Image

    img, target, img_id = val_dataset[idx]
    # Load image information using img_id
    img_info = val_dataset.coco.loadImgs(img_id)[0]
    img_path = os.path.join(val_images_dir, img_info['file_name'])
    img_pil = Image.open(img_path).convert("RGB")

    # Move image to device and add batch dimension
    img_tensor = img.to(device)
    img_batch = [img_tensor]

    # Get prediction
    model.eval()
    with torch.no_grad():
        prediction = model(img_batch)[0]

    return img_pil, target, prediction

# Assuming the first few images have good predictions
idx = 0  # You can change this index
img_pil, target, prediction = get_image_and_prediction(idx)

# Plot the image with ground truth and predicted boxes
plot_image_with_boxes(img_pil, targets=target, predictions=prediction, score_threshold=0.5)

from torchvision.ops import box_iou

def plot_detailed_predictions(img, target, prediction, iou_threshold=0.5):
    if isinstance(img, torch.Tensor):
        img = img.permute(1, 2, 0).cpu().numpy()

    fig, ax = plt.subplots(1, figsize=(12, 8))
    ax.imshow(img)

    # Plot ground truth boxes
    gt_boxes = []
    for ann in target:
        bbox = ann['bbox']
        category_id = ann['category_id']
        category_name = coco_gt.loadCats(category_id)[0]['name']
        rect = patches.Rectangle(
            (bbox[0], bbox[1]), bbox[2], bbox[3],
            linewidth=2, edgecolor='green', facecolor='none'
        )
        ax.add_patch(rect)
        ax.text(
            bbox[0], bbox[1] - 5, category_name,
            color='green', fontsize=12, backgroundcolor='black'
        )
        gt_boxes.append([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]])
    gt_boxes = torch.tensor(gt_boxes)

    # Predictions
    pred_boxes = prediction['boxes'].cpu()
    pred_scores = prediction['scores'].cpu()
    pred_labels = prediction['labels'].cpu()

    # Compute IoU
    ious = box_iou(pred_boxes, gt_boxes)

    # For each predicted box, find the best matching ground truth box
    iou_max, gt_indices = ious.max(dim=1)

    for idx, (bbox, score, label) in enumerate(zip(pred_boxes, pred_scores, pred_labels)):
        if score < 0.5:
            continue
        bbox = bbox.numpy()
        category_name = coco_gt.loadCats(int(label))[0]['name']
        if iou_max[idx] >= iou_threshold:
            edgecolor = 'blue'  # True positive
        else:
            edgecolor = 'red'  # False positive

        rect = patches.Rectangle(
            (bbox[0], bbox[1]), bbox[2] - bbox[0], bbox[3] - bbox[1],
            linewidth=2, edgecolor=edgecolor, facecolor='none'
        )
        ax.add_patch(rect)
        ax.text(
            bbox[0], bbox[1] - 5, f'{category_name}: {score:.2f}',
            color=edgecolor, fontsize=12, backgroundcolor='black'
        )

    plt.axis('off')
    plt.show()

plot_detailed_predictions(img_pil, target=target, prediction=prediction, iou_threshold=0.5)

