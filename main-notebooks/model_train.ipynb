{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOATFzycZETt"
      },
      "source": [
        "## Run Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2NcuiVOZHHN"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4r3xFKc6Yoet"
      },
      "outputs": [],
      "source": [
        "# To be able to import .py scripts stored under src/\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks/cmpe593/term-project/src')\n",
        "import os\n",
        "import torch\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "# Run parameters\n",
        "model_type = \"custom_FADE\" # TODO: custom_FADE or \"baseline\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "##############################################################################\n",
        "# PATHS\n",
        "##############################################################################\n",
        "base_path = \"/content/drive/MyDrive\"\n",
        "project_dir = os.path.join(base_path, \"Colab Notebooks\", \"cmpe593\", \"term-project\")\n",
        "\n",
        "## Dataset paths\n",
        "dataset_split = \"train2017\" # TODO: pick from (\"train2017\", \"val2017\")\n",
        "coco_base_dir = f'{base_path}/coco'\n",
        "images_dir = os.path.join(coco_base_dir, 'images', dataset_split)\n",
        "annotations_path = os.path.join(coco_base_dir,\n",
        "                                'annotations',\n",
        "                                f'instances_{dataset_split}.json')\n",
        "\n",
        "## Artifact paths (checkpoints, plots, metrics)\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "results_dir = os.path.join(project_dir, 'results', timestamp)\n",
        "checkpoint_dir = os.path.join(results_dir, 'checkpoints')\n",
        "plot_dir = os.path.join(results_dir, 'plots')\n",
        "metrics_dir = os.path.join(results_dir, 'metrics')\n",
        "\n",
        "\n",
        "# Ensure artifact paths exist\n",
        "for dir in [results_dir, checkpoint_dir, plot_dir, metrics_dir]:\n",
        "  if not os.path.exists(dir):\n",
        "    os.makedirs(dir)\n",
        "    print(f'Directory created at {dir}')\n",
        "  else:\n",
        "    print(f'Directory {dir} already exists.')\n",
        "##############################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbCJxQTXyp4G"
      },
      "source": [
        "## Load data and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fc7LWgf1dEN0"
      },
      "outputs": [],
      "source": [
        "from dataops.data_loader import get_data_loader\n",
        "from dataops.utils import get_num_classes\n",
        "\n",
        "# Get data loader and number of classes inside the dataset\n",
        "dataloader = get_data_loader(images_dir, annotations_path, train=True, subset_size=10000)\n",
        "num_classes, category_ids = get_num_classes(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKsOalAocQYp"
      },
      "outputs": [],
      "source": [
        "from modelops import model_loader\n",
        "\n",
        "model = model_loader.load_model(\n",
        "    model_type=model_type,\n",
        "    device=device,\n",
        "    eval_mode=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50USW5pQ8iAo"
      },
      "source": [
        "## Configure Model and Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsJMwYFzXmwk"
      },
      "source": [
        "### Set parameters and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPzxwTn142EU"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "# Set hyperparameters\n",
        "num_classes = len(category_ids) + 1  # +1 for background\n",
        "quick_experiment = False\n",
        "hyperparam_key = 'quick_experiment' if quick_experiment else 'longer_experiment'\n",
        "\n",
        "hyperparameters = {\n",
        "    # Quick experiment is for sanity check\n",
        "    'quick_experiment': {\n",
        "        'lr_scheduler': None,\n",
        "        'lr': 0.01,\n",
        "        'momentum': 0.9,\n",
        "        'weight_decay': 0.0005,\n",
        "        'num_epochs': 3,\n",
        "    },\n",
        "\n",
        "    'longer_experiment': {\n",
        "        'lr_scheduler': 'scheduledLRonly',\n",
        "        'init_lr': 0.01,\n",
        "        'momentum': 0.9,\n",
        "        'weight_decay': 0.0005,\n",
        "        'num_epochs': 20,\n",
        "    },\n",
        "}\n",
        "\n",
        "# Extract hyperparameters\n",
        "active_hparams = hyperparameters[hyperparam_key]\n",
        "lr = active_hparams['init_lr']\n",
        "momentum = active_hparams['momentum']\n",
        "weight_decay = active_hparams['weight_decay']\n",
        "num_epochs = active_hparams['num_epochs']\n",
        "\n",
        "# Initialize model parameters that require gradients\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.SGD(\n",
        "    params,\n",
        "    lr=lr,\n",
        "    momentum=momentum,\n",
        "    weight_decay=weight_decay\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-Lm0GYMXutR"
      },
      "source": [
        "### Save params for debugging purposes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZKqIqvwXRsx"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Path to save the hyperparameters file\n",
        "hyperparams_path = os.path.join(results_dir, \"hyperparameters.json\")\n",
        "active_hparams['model_type'] = f'{model_type}-{model.__class__.__name__}'\n",
        "\n",
        "# Save hyperparameters\n",
        "with open(hyperparams_path, \"w\") as f:\n",
        "    json.dump(active_hparams, f, indent=4)\n",
        "\n",
        "print(f\"Hyperparameters saved to {hyperparams_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RkUKSgqOeFq"
      },
      "source": [
        "### Define training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDACYS3HPNhL"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import shutil\n",
        "import datetime\n",
        "from tqdm import tqdm\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    optimizer,\n",
        "    device,\n",
        "    num_epochs,\n",
        "    drive_checkpoint_dir,\n",
        "    lr_scheduler=None,\n",
        "    val_loader=None,\n",
        "):\n",
        "  epoch_losses = []\n",
        "  error_batches = []\n",
        "\n",
        "  # File paths for saving loss data\n",
        "  epoch_loss_file = os.path.join(drive_checkpoint_dir, \"epoch_losses.txt\")\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    batch_count = 0\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}: Learning Rate = {optimizer.param_groups[0]['lr']}\")\n",
        "    print('-' * 20)\n",
        "\n",
        "    # Training loop\n",
        "    for images, targets, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}\", unit=\"batch\"):\n",
        "      batch_count += 1\n",
        "\n",
        "      # Move images and targets to the device\n",
        "      images = [img.to(device) for img in images]\n",
        "      targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "      try:\n",
        "        # Forward pass\n",
        "        loss_dict = model(images, targets)\n",
        "      except AssertionError as e:\n",
        "        print(f\"\\n[ERROR] Bounding box error in batch {batch_count}!\")\n",
        "        batch_error_info = {\"epoch\": epoch+1, \"batch_idx\": batch_count, \"boxes\": []}\n",
        "\n",
        "        for i, tgt in enumerate(targets):\n",
        "          batch_error_info[\"boxes\"].append(tgt['boxes'].cpu().tolist())\n",
        "\n",
        "        error_batches.append(batch_error_info)\n",
        "        continue\n",
        "\n",
        "      # Compute total loss\n",
        "      losses = sum(loss for loss in loss_dict.values())\n",
        "      loss_value = losses.item()\n",
        "      epoch_loss += loss_value\n",
        "\n",
        "      # Backward pass and optimization\n",
        "      optimizer.zero_grad()\n",
        "      losses.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Display intermediate losses\n",
        "      if batch_count % 250 == 0:\n",
        "        print(f\"Batch {batch_count}, Loss: {loss_value:.4f}\")\n",
        "\n",
        "    # Step the learning rate scheduler\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    if lr_scheduler:\n",
        "      print(f\"At epoch {epoch + 1} stepping from learning rate: {current_lr:.6f}\")\n",
        "      lr_scheduler.step()\n",
        "      print(f\"At epoch {epoch + 1} stepping to learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "    epoch_duration = time.time() - start_time\n",
        "    average_epoch_loss = epoch_loss / len(train_loader)\n",
        "    epoch_losses.append(average_epoch_loss)\n",
        "\n",
        "    with open(epoch_loss_file, \"a\") as f:\n",
        "        # Save epoch summary\n",
        "        epoch_summary = (\n",
        "            f\"Epoch [{epoch + 1}/{num_epochs}] completed in {str(datetime.timedelta(seconds=int(epoch_duration)))}\\n\"\n",
        "            f\"Learning Rate: \"\n",
        "            f\"Average Loss: {average_epoch_loss:.4f}\\n\"\n",
        "        )\n",
        "\n",
        "        # Print to console\n",
        "        print(epoch_summary.strip())\n",
        "\n",
        "        # Write to file\n",
        "        f.write(epoch_summary)\n",
        "\n",
        "    # Save checkpoint\n",
        "    drive_checkpoint_path = os.path.join(drive_checkpoint_dir, f'model_epoch_{epoch + 1}.pth')\n",
        "    torch.save(model.state_dict(), drive_checkpoint_path)\n",
        "    print(f\"Checkpoint saved to Google Drive at {drive_checkpoint_path}\")\n",
        "\n",
        "  # =========================\n",
        "  # Training loop completes\n",
        "  # =========================\n",
        "  print(\"Training loop finished.\")\n",
        "\n",
        "  # Save the model state dictionary\n",
        "  drive_final_checkpoint_path = os.path.join(drive_checkpoint_dir, 'model_final.pth')\n",
        "  torch.save(model.state_dict(), drive_final_checkpoint_path)\n",
        "  print(f\"Final model saved to Google Drive at {drive_final_checkpoint_path}\")\n",
        "\n",
        "  # Save and display the training loss plot\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  plot_filename_drive = os.path.join(plot_dir, 'training_loss_plot.png')\n",
        "  plt.figure()\n",
        "  plt.plot(range(1, num_epochs + 1), epoch_losses, marker='o')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Average Loss')\n",
        "  plt.title('Training Loss Over Epochs')\n",
        "  plt.grid(True)\n",
        "  plt.savefig(plot_filename_drive)\n",
        "  print(f\"Training loss plot saved to Google Drive at {plot_filename_drive}\")\n",
        "  plt.show()\n",
        "\n",
        "  if error_batches:\n",
        "      print(\"\\nSome batches triggered bounding box assertions. Their info:\")\n",
        "      for info in error_batches:\n",
        "          print(f\"Epoch {info['epoch']}, Batch {info['batch_idx']}, Boxes: {info['boxes']}\")\n",
        "  else:\n",
        "      print(\"No bounding box errors encountered!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B_HbxTO-svP"
      },
      "source": [
        "## Initiate Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xz9IwfpY4ck"
      },
      "outputs": [],
      "source": [
        "def lr_schedule(epoch):\n",
        "    epoch += 1\n",
        "    lr = 1.0\n",
        "\n",
        "    if epoch in [1]:\n",
        "      lr = 1.0\n",
        "    elif epoch in [2, 3]:\n",
        "      lr = 0.4\n",
        "    elif epoch in [4, 5]:\n",
        "      lr = 0.1\n",
        "    elif epoch in [6, 7]:\n",
        "      lr = 0.03\n",
        "    elif epoch in [8, 9]:\n",
        "      lr = 0.01\n",
        "    else:\n",
        "      lr = 0.001\n",
        "\n",
        "    return lr\n",
        "\n",
        "\n",
        "# Apply the scheduler\n",
        "lr_scheduler = LambdaLR(optimizer, lr_lambda=lr_schedule)\n",
        "\n",
        "train_model(\n",
        "    model=model,\n",
        "    train_loader=dataloader,\n",
        "    optimizer=optimizer,\n",
        "    device=device,\n",
        "    num_epochs=num_epochs,\n",
        "    drive_checkpoint_dir=checkpoint_dir,\n",
        "    lr_scheduler=lr_scheduler,\n",
        "    val_loader=None\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}